{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model-based Regularization\n",
    "\n",
    "Now that we know the main caveats of solving inverse problems, it's time to find out how to tackle them! The keyword here will be **regularization**. A regularization method is usually adapted to the noise level $\\delta$ with a regularization parameter $\\lambda > 0$ and has two essential properties:\n",
    "* The regularization method is **continuous** (i.e., stable) for all choices of $\\lambda$.\n",
    "* As the noise level tends to zero, the regularization method **converges pointwise to a generalized inverse** (e.g., the operator that maps a noisy measurement to the minimum norm solution).\n",
    "\n",
    "A popular class of regularization methods can be formulated as a variational problem, where the reconstruction $x^*$ of a noisy measurement $y^{\\delta}$ is obtained as the solution of\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2_2 + \\lambda J(x), $$\n",
    "where the functional $J$ is called the regularization functional and penalizes unwanted behaviour of $x$. In this part of the tutorial, we will see what typical examples of $J$ look like and get an idea of how the minimizer of the above problem can be found. For a stochastical interpretation of the variational formulation stay tuned for the last part of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "import operators\n",
    "import optimizers\n",
    "import numpy as np\n",
    "import pywt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization\n",
    "\n",
    "The first regularization functional we consider is $J(x) = \\frac{1}{2}\\|x\\|^2_2 $. An easy interpretation of this would be that **penalizing reconstructions with a large norm prevents the error from exploding** (compare Task 1.4). The resulting variational problem then reads\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2_2 + \\frac{\\lambda}{2} \\|x\\|^2_2.$$\n",
    "\n",
    "Analytically, the solution of the above problem can be computed as \n",
    "$$ x^* = (A^*A + \\lambda \\operatorname{Id})^{-1} A^*y^\\delta,$$\n",
    "where $A^*$ denotes the adjoint of $A$ and $\\operatorname{Id}$ denotes the identity operator. In this form, the method is also known as Tikhonov regularization.\n",
    "\n",
    "Since we cannot easily access $(A^*A + \\lambda \\operatorname{Id})^{-1}$, we can perform a simple gradient descent with stepsize $t > 0$ of the form\n",
    "$$ x \\leftarrow x - t \\cdot (A^*Ax + \\lambda x - A^*y^\\delta )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Task 2.1 </span>\n",
    "#### Complete the following algorithm that performs gradient descent to solve the Tikhonov regularization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gradient_descent(optimizers.optimizer):\n",
    "    def __init__(self, A, x, y, t=0.1, lamda=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.A = A\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        self.lamda = lamda\n",
    "        def energy_fun(x):\n",
    "            return 0.5* np.linalg.norm(A(x)-y)**2 + lamda*0.5*np.linalg.norm(x, ord=1)\n",
    "        self.energy_fun=energy_fun\n",
    "\n",
    "    def step(self,):\n",
    "           \n",
    "        ###### TODO's start here #####\n",
    "\n",
    "        grad = self.A.adjoint(self.A(self.x)) + self.lamda*self.x - self.A.adjoint(self.y) # None # TODO grad should be the gradient A*Ax + \\lambda x - A*y^\\delta.\n",
    "                                                                                                  # You can assume that A* the adjoint of self.A is implemented as self.A.adjoint\n",
    "        self.x = self.x - self.t*grad # None # TODO update self.x by performing a gradient descent with stepsize 'self.t'\n",
    "\n",
    "        ###### TODO's start here #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478ee1149b044b5bda891a87346ca61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='lamda', max=10.0, step=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim = 128\n",
    "pre_compute_tikh = {}\n",
    "phantom = utils.get_phantom(dim)\n",
    "\n",
    "def plot_tikh_reco(lamda, noise_lvl, max_angle):\n",
    "    num_theta = int(np.ceil(max_angle/180*dim))\n",
    "    theta = np.linspace(0,max_angle, endpoint= False, num=num_theta)\n",
    "    R = operators.Radon(theta=theta)\n",
    "    if (lamda, noise_lvl, max_angle) in pre_compute_tikh:\n",
    "        x = pre_compute_tikh[(lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        sinogram = R(phantom) + np.random.normal(loc = 0, scale= noise_lvl, size = [dim,num_theta])\n",
    "        \n",
    "        gd = gradient_descent(R, R.inv(sinogram), sinogram, t=1/(noise_lvl*lamda*10000) if (noise_lvl*lamda > 0 ) else 0.0001, lamda=lamda, verbosity = 0)\n",
    "        gd.solve()\n",
    "        \n",
    "        \n",
    "        x = gd.x\n",
    "        pre_compute_tikh[(lamda, noise_lvl, max_angle)] = x\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax = 1)\n",
    "\n",
    "l_slider = widgets.FloatSlider(min = 0, max = 10., step = 1, value = 0, continuous_update = False)\n",
    "s_slider = widgets.FloatSlider(min = 0.001, max = .01, step = 0.001, value = 0.001, continuous_update = False)\n",
    "t_slider = widgets.IntSlider(min = 1, max = 180, step = 10, value = 180, continuous_update = False)\n",
    "interactive_plot = interactive(plot_tikh_reco, lamda = l_slider, noise_lvl = s_slider, max_angle= t_slider)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity Promoting Regularization\n",
    "Another demand on the regularization method could be that the obtained **reconstructions should be simple in a certain sense**. This means that they should be composed of only a few building blocks. \n",
    "\n",
    "The two ingredients we will need to enforce this are\n",
    "* an operator $D$ which decomposes $x$ into building blocks, for example a wavelet decomposition\n",
    "* the $\\|\\cdot\\|_1$-norm which promotes sparsity, i.e., penalizes non-zero entries of $Dx$.\n",
    "\n",
    "The resulting variational problem then reads\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2_2 + \\lambda \\|Dx\\|_1.$$\n",
    "\n",
    "Since the regularization functional is in general not differentiable, we will use the so-called proximal gradient descent algorithm. The proximal map of a functional $J$ with parameter $\\lambda$ is defined as\n",
    "$$ \\operatorname{prox}_{\\lambda J}(x) = \\operatorname*{arg\\ min}_z \\frac{1}{2}\\|x-z\\|_2^2 + \\lambda J(z).$$\n",
    "\n",
    "The proximal gradient descent update then reads \n",
    "$$ x \\leftarrow \\operatorname{prox}_{t\\lambda J} (x - 2t \\cdot (A^*Ax - A^*y^\\delta )).$$\n",
    "\n",
    "\n",
    "Fortunately, the proximal map of the functionals we consider now is easy to compute with the help of the soft-shrinkage function, which is in fact the proximal map of the $\\|\\cdot\\|_1$-norm. For different choices of $\\lambda$ it looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905408fdc0bc4dffb0fda3b79fa63cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='lamda', max=10.0, step=0.5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = np.linspace(-10,10, num=100)\n",
    "def plot_soft(lamda):\n",
    "    w = optimizers.soft_shrinkage(v, lamda)\n",
    "    plt.plot(v,w)\n",
    "    plt.ylim([-10,10])\n",
    "    plt.title('$prox_{\\lambda \\|\\cdot\\|_1}$, $\\lambda = $' +str(lamda))\n",
    "    \n",
    "    \n",
    "slider = widgets.FloatSlider(min = 0, max = 10, step = 0.5, value = 1, continuous_update = True)\n",
    "interactive_plot = interactive(plot_soft, lamda = slider)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Task 2.2 </span>\n",
    "#### How does the soft-shrinkage function enforce sparsity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Task 2.3 </span>\n",
    "#### Try out sparsity promoting regularization for different choices of $D$ (Identity, Haar-Wavelet Transform and Daubechies4-Wavelet Transform) and describe how the reconstructions change depending on $D$.\n",
    "To interpret the approaches using wavelet decompositions it might be useful to look for an image of the corresponding wavelets online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328d62563ce14c61b1b5214a12116e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Decomposing operator D', options=(('Identity', 0), ('Haar-Wav…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim = 128\n",
    "pre_compute_sparse = {}\n",
    "phantom = utils.get_phantom(dim)\n",
    "\n",
    "def plot_sparse_reco(D_idx, lamda, noise_lvl, max_angle):\n",
    "    num_theta = int(np.ceil(max_angle/180*dim))\n",
    "    theta = np.linspace(0,max_angle, endpoint= False, num=num_theta)\n",
    "    R = operators.Radon(theta=theta)\n",
    "    if (D_idx, lamda, noise_lvl, max_angle) in pre_compute_sparse:\n",
    "        x = pre_compute_sparse[(D_idx, lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        sinogram = R(phantom) + np.random.normal(loc = 0, scale= noise_lvl, size = [dim,num_theta])\n",
    "        opti = None\n",
    "        if D_idx == 0:\n",
    "            opti = optimizers.ista_L1(R, R.inv(sinogram), sinogram, t=1/(noise_lvl*lamda*10000) if (noise_lvl*lamda > 0 ) else 0.0001, lamda=lamda,  verbosity = 0)\n",
    "        elif D_idx == 1:\n",
    "            opti = optimizers.ista_wavelets(R, R.inv(sinogram), sinogram, wave = pywt.Wavelet('haar'), t=1/(noise_lvl*lamda*10000) if (noise_lvl*lamda > 0 ) else 0.0001, lamda=lamda,  verbosity = 0)\n",
    "        elif D_idx == 2:\n",
    "            opti = optimizers.ista_wavelets(R, R.inv(sinogram), sinogram, wave = pywt.Wavelet('db4'), t=1/(noise_lvl*lamda*10000) if (noise_lvl*lamda > 0 ) else 0.0001, lamda=lamda,  verbosity = 0)\n",
    "\n",
    "        opti.solve()\n",
    "        \n",
    "        x = opti.x\n",
    "\n",
    "        pre_compute_sparse[(D_idx, lamda, noise_lvl, max_angle)] = x\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax = 1)\n",
    "\n",
    "idx_toggle = widgets.ToggleButtons(\n",
    "    options=[('Identity',0), ('Haar-Wavelet',1), ('Daubechies4-Wavelet',2)],\n",
    "    description='Decomposing operator D',\n",
    "    disabled=False\n",
    ")\n",
    "l_slider = widgets.FloatSlider(min = 0, max = 0.5, step = 0.01, value = 0, continuous_update = False)\n",
    "s_slider = widgets.FloatSlider(min = 0.001, max = .01, step = 0.001, value = 0.001, continuous_update = False)\n",
    "t_slider = widgets.IntSlider(min = 1, max = 180, step = 10, value = 180, continuous_update = False)\n",
    "interactive_plot = interactive(plot_sparse_reco, D_idx = idx_toggle, lamda = l_slider, noise_lvl = s_slider, max_angle= t_slider)\n",
    "\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation Regularization\n",
    "\n",
    "A regularization functional that works particularly well for image processing is the Total Variation (TV). It can be written as\n",
    "$$\\operatorname{TV}(x) = \\| \\nabla x\\|_1$$ \n",
    "and is thus also a sparsity promoting functional. However, we cannot compute its proximal map as easy as above and thus have to use a more complicated optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Task 2.4 </span>\n",
    "#### Try to find out in which sense TV regularization promotes sparsity. The following slider showing some TV reconstructions might help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868baf70beda4e3482cf18a259b4bd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='lamda', max=0.1, step=0.001…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim = 128\n",
    "pre_compute_tv = {}\n",
    "phantom = utils.get_phantom(dim)\n",
    "\n",
    "def plot_tv_reco(lamda, noise_lvl, max_angle):\n",
    "    num_theta = int(np.ceil(max_angle/180*dim))\n",
    "    theta = np.linspace(0,max_angle, endpoint= False, num=num_theta)\n",
    "    R = operators.Radon(theta=theta)\n",
    "    if (lamda, noise_lvl, max_angle) in pre_compute_sparse:\n",
    "        x = pre_compute_sparse[(lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        sinogram = R(phantom) + np.random.normal(loc = 0, scale= noise_lvl, size = [dim,num_theta])\n",
    "        def energy_fun(x):\n",
    "            return np.linalg.norm(R(x) - sinogram)**2 + lamda * operators.TV()(x)\n",
    "        \n",
    "        sBTV = optimizers.split_Bregman_TV(R, sinogram, R.inv(sinogram), gamma=1.0, \n",
    "                        energy_fun=energy_fun, lamda = lamda,\n",
    "                        max_it = 10,\n",
    "                        max_inner_it = 2, verbosity = 0)\n",
    "        sBTV.solve()\n",
    "        \n",
    "        x = sBTV.x\n",
    "\n",
    "        pre_compute_tv[(lamda, noise_lvl, max_angle)] = x\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax = 1)\n",
    "\n",
    "l_slider = widgets.FloatSlider(min = 0, max = 0.1, step = 0.001, value = 0, continuous_update = False)\n",
    "s_slider = widgets.FloatSlider(min = 0.001, max = .01, step = 0.001, value = 0.001, continuous_update = False)\n",
    "t_slider = widgets.IntSlider(min = 1, max = 180, step = 10, value = 180, continuous_update = False)\n",
    "interactive_plot = interactive(plot_tv_reco, lamda = l_slider, noise_lvl = s_slider, max_angle= t_slider)\n",
    "\n",
    "display(interactive_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
