{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model-based Regularization\n",
    "\n",
    "Now that we have learned the main caveats of solving inverse problems, it's time to find out how to tackle them! The keyword here will be **regularization**. A regularization method is usually adapted to the noise level $\\delta$ with a regularization parameter $\\lambda > 0$ and has two essential properties:\n",
    "* The regularization method is **continuous** (i.e., stable) for all choices of $\\lambda$.\n",
    "* As the noise level tends to zero, the regularization method **converges pointwise to a generalized inverse** (e.g., the operator that maps a noisy measurement to the minimum norm solution).\n",
    "\n",
    "A popular class of regularization methods can be formulated as a variational problem, where the reconstruction $x^*$ of a noisy measurement $y^{\\delta}$ is obtained as the solution of\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2 + \\lambda J(x), $$\n",
    "where the functional $J$ is called the regularization functional and penalizes unwanted behaviour of $x$. In this part of the tutorial, we will see what typical examples of $J$ look like and get an idea of how the minimizer of the above problem can be found. For a stochastical interpretation of the variational formulation stay tuned for the last part of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
