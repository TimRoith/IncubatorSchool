{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model-based Regularization\n",
    "\n",
    "Now that we have learned the main caveats of solving inverse problems, it's time to find out how to tackle them! The keyword here will be **regularization**. A regularization method is usually adapted to the noise level $\\delta$ with a regularization parameter $\\lambda > 0$ and has two essential properties:\n",
    "* The regularization method is **continuous** (i.e., stable) for all choices of $\\lambda$.\n",
    "* As the noise level tends to zero, the regularization method **converges pointwise to a generalized inverse** (e.g., the operator that maps a noisy measurement to the minimum norm solution).\n",
    "\n",
    "A popular class of regularization methods can be formulated as a variational problem, where the reconstruction $x^*$ of a noisy measurement $y^{\\delta}$ is obtained as the solution of\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2_2 + \\lambda J(x), $$\n",
    "where the functional $J$ is called the regularization functional and penalizes unwanted behaviour of $x$. In this part of the tutorial, we will see what typical examples of $J$ look like and get an idea of how the minimizer of the above problem can be found. For a stochastical interpretation of the variational formulation stay tuned for the last part of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization\n",
    "\n",
    "The first regularization functional we consider is $J(x) = \\frac{1}{2}\\|x\\|^2_2 $. An easy interpretation of this would be that **penalizing reconstructions with a large norm prevents the error from exploding** (compare Task 1.4). The resulting variational problem then reads\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\delta}\\|^2_2 + \\frac{\\lambda}{2} \\|x\\|^2_2.$$\n",
    "\n",
    "Analytically, the solution of the above problem can be computed as \n",
    "$$ x^* = (A^*A + \\lambda \\operatorname{Id})^{-1} A^*y,$$\n",
    "where $A^*$ denotes the adjoint of $A$ and $\\operatorname{Id}$ denotes the identity operator. \n",
    "\n",
    "Since we cannot easily access $(A^*A + \\lambda \\operatorname{Id})^{-1}$, we can perform a simple gradient descent with stepsize $t > 0$ of the form\n",
    "$$ x \\leftarrow x - t \\cdot (A^*Ax + \\lambda x - A^*y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128221; <span style=\"color:darkorange\"> Task 2.1 </span>\n",
    "#### Complete the following "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
